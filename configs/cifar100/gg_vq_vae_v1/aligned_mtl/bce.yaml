# CIFAR100 GG-VQ-VAE Configuration - AlignedMTL Aggregator (BCE)
# This configuration trains a GG-VQ-VAE on CIFAR100 dataset with BCE mean objectives

dataset: cifar100
arch: gg_vq_vae
epochs: 100
batch_size: 128
optimizer: adam
lr: 3e-4
wd: 0.0
aggregator: aligned_mtl
normalize: false

# Model architecture
embedding_dim: 64
num_embeddings: 512
hidden_dims: [128, 256]
loss_weights: [0.307692, 0.307692, 0.076923, 0.307692]
recons_dist: bernoulli
recons_reduction: mean

# Training settings
seed: 42
device: cuda:0
data_dir: ../data
save_path: logs/
num_workers: 0
save_freq: 10
eval_freq: 1
num_samples: 64

# Hypervolume reference points (GG-VQ-VAE v2 has 5 objectives: reconstruction_loss, embedding_loss, commitment_loss, gradient_guided_loss)
hv_ref: [1.1, 1.1, 1.1, 1.1]

# Wandb settings
use_wandb: true
wandb_project: mo-vae
wandb_entity: rasa_research
wandb_name: "cifar100 gg_vq_vae aligned_mtl bernoulli 512k 64d seed42"



