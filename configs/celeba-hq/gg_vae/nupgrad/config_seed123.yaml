# CelebA-HQ GG-VAE Configuration - NUpGrad Aggregator (BCE)
# This configuration trains a GG-VAE on CelebA-HQ dataset with BCE mean objectives
# Dataset settings

# Dataset settings
dataset: CelebA-HQ
data_dir: ../data
normalize: false
# Model architecture
arch: gg_vae
latent_dim: 128
hidden_dims: [32, 64, 128, 256, 512]
loss_weights: [1.0, 8.138e-05, 1.0, 1.0]
recons_dist: bernoulli
recons_reduction: mean
# Training settings
epochs: 100
batch_size: 64
optimizer: adamw
lr: 3e-4
wd: 0.0
aggregator: nupgrad
seed: 123
device: cuda:0
num_workers: 8
# Logging settings
save_path: logs/
save_freq: 10
eval_freq: 1
num_samples: 4
# Hypervolume reference points (GG-VAE has 4 objectives: reconstruction_loss, kl_loss, gradient_guided_loss, edge_matching_loss)
hv_ref: [1.1, 1.1, 1.1, 1.1]
# Wandb settings
use_wandb: true
wandb_project: mo-vae
wandb_entity: rasa_research
wandb_name: celeba-hq gg_vae 128d nupgrad [1*bce + 8.138e-05*kld + 1*gradient_guided + 1*edge_matching] seed123

