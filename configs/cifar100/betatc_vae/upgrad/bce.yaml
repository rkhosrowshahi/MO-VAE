# CIFAR100 BetaTC-VAE Configuration - UPGrad Aggregator (BCE)
# This configuration trains a BetaTC-VAE on CIFAR100 dataset with BCE reconstruction
# BetaTC-VAE decomposes ELBO into: reconstruction_loss, mi_loss, tc_loss, kld
# Note: BetaTCVAE currently uses MSE loss; this config is for future BCE support

dataset: cifar100
data_dir: ../data
arch: betatc_vae
epochs: 100
batch_size: 128
optimizer: adam
lr: 0.001
wd: 0.0
aggregator: upgrad
normalize: false

# Model architecture
latent_dim: 128
hidden_dims: [32, 32, 64, 64]
loss_weights: [1.0, 1.0, 6.0, 1.0]
# BetaTC-VAE specific parameters
anneal_steps: 10000
recons_dist: bernoulli
recons_reduction: mean

# Training settings
seed: 42
device: cuda:0
save_path: logs/
save_freq: 10
eval_freq: 1
num_samples: 64

# Hypervolume reference points (BetaTC-VAE has 4 objectives: reconstruction_loss, mi_loss, tc_loss, kld)
hv_ref: [1.1, 1.1, 1.1, 1.1]

# Wandb settings (optional)
use_wandb: true
wandb_project: mo-vae
wandb_entity: rasa_research
wandb_name: "cifar100 128d betatc_vae bce upgrad"

