# CELEBA BetaTC-VAE Configuration - CAGrad Aggregator
# This configuration trains a BetaTC-VAE on CELEBA dataset


# Dataset settings
dataset: CelebA
data_dir: ../data
normalize: true
# Model architecture
arch: betatc_vae
latent_dim: 128
hidden_dims: [32, 32, 64, 64]
loss_weights: [1.0, 1.0, 6.0, 1.0]
recons_dist: gaussian
recons_reduction: mean
# Training settings
epochs: 100
batch_size: 128
optimizer: adam
lr: 0.001
wd: 0.0
aggregator: cagrad
seed: 999
device: cuda:0
num_workers: 4
# Logging settings
save_path: logs/
save_freq: 10
eval_freq: 10
num_samples: 64
# Hypervolume reference points (BetaTC-VAE has 2 objectives: reconstruction_loss, tc_loss)
hv_ref: [1.1, 1.1, 1.1, 1.1]
# Wandb settings
use_wandb: true
wandb_project: mo-vae
wandb_entity: rasa_research
wandb_name: celeba 128d betatc_vae CAGrad seed999
anneal_steps: 10000

